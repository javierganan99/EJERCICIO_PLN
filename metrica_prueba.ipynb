{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8931e4d8",
   "metadata": {},
   "source": [
    "## Apartado 1\n",
    "\n",
    "En este primer ejercicio se implementa la función *string addPunctuationBasic(string)* que, dado un string de entrada que contiene palabras en minísculas y sin signos de puntuación, devuelve dicho string con la primera letra en mayúsculas y un punto al final. Para mayor generalidad, consideramos que la frase pueda contener espacios o tabulaciones al final, y si ya contiene un punto final no añadimos otro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1843ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm Francisco Javier Gañán.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addPunctuationBasic(sent):\n",
    "    sent = sent[0].upper() + sent[1:]\n",
    "    while sent[-1] == ' ' or sent[-1] == '\\t' or sent[-1] == '\\n':\n",
    "        sent = sent[0:-1]\n",
    "    if sent[-1] == '.' or sent[-1] == ',' or sent[-1] == '.' or sent[-1] == ':' or sent[-1] == ';' or sent[-1] == '?' or sent[-1] == '!':\n",
    "        return sent\n",
    "    else:\n",
    "        return sent + '.'\n",
    "\n",
    "addPunctuationBasic(\"i'm Francisco Javier Gañán\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e352673",
   "metadata": {},
   "source": [
    "## Apartado 2\n",
    "\n",
    "En este segundo apartado se implementa la función *[(pos, err)] verifyPunctuation(string test, string check)*. Esta función recibe como entrada dos strings y, basándose en la distancia de Levenshtein y en recorrer la matriz que se genera, calcula una lista de mínimas ediciones para convertir *test* en *check*. Se puede encontrar información sobre este algoritmo para calcular la distancia de Levenshtein y la matriz asociada en https://sites.google.com/site/algoritmossimilaridaddistancia/distancia-de-levenshtein. \n",
    "\n",
    "En el modelo original de Levenshtein, que es el se usa en este apartado, cada operación de edición (inserción, sustitución o eliminación) tiene coste 1. Por tanto, la distancia de Levenshtein es la suma del número de ediciones realizadas. Needleman y Wunsch, en 1970, lo modificaron para permitir operaciones de edición con distinto costo. Sin embargo, esto no se considera en este ejercicio, ya que, en general, debería asignarse un costo personalizado a cada pareja de palabras para sustitución, eliminación e inserción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818b280",
   "metadata": {},
   "source": [
    "### Tokenizar\n",
    "\n",
    "La siguiente función (*Tokenize*) se utiliza para tokenizar. Funciona tanto para frases individuales como para textos completos con numerosas frases. Se ha diseñado así para poder tokenizar correctamente los corpus utilizados en este ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(seq):\n",
    "        seqToken = [] # Guarda la secuencia tokenizada\n",
    "        ant = 0 # Guarda la posición del carácter desde el que comienza un nuevo token\n",
    "        punt_tuple = (\".\", \",\", \":\", \";\", \"?\", \"!\")\n",
    "        for i in range(len(seq)):\n",
    "            if ant <= i:\n",
    "                if seq[i] == ' ' and not (seq[i-1] in punt_tuple or seq[i-1] == ' ' or seq[i-1] == '\\n'):    \n",
    "                    seqToken.append(seq[ant:i])\n",
    "                    ant = i + 1\n",
    "                elif seq[i] == ' ':\n",
    "                    ant = i + 1\n",
    "                elif seq[i] in punt_tuple:\n",
    "                    if ant < i:\n",
    "                        seqToken.append(seq[ant:i])\n",
    "                        seqToken.append(seq[i])\n",
    "                        ant = i + 1\n",
    "                    else:\n",
    "                        seqToken.append(seq[i])\n",
    "                        ant = i + 1\n",
    "                elif i == len(seq) - 1 and seq[i] != '\\n':\n",
    "                    seqToken.append(seq[ant:i+1])\n",
    "\n",
    "                elif seq[i].isnumeric():\n",
    "                    j = i\n",
    "                    while j < len(seq) and seq[j] != \" \": #seq[j] in punt_tuple or seq[j].isnumeric():\n",
    "                        j += 1\n",
    "                    if seq[j-1] not in punt_tuple:\n",
    "                        seqToken.append(\"$NUMERIC\")\n",
    "                    else:\n",
    "                        seqToken.append(\"$NUMERIC\")\n",
    "                        seqToken.append(seq[j-1])\n",
    "                    ant = j + 1\n",
    "                # Para leer textos completos considerando el salto de línea. El salto de línea no se considera como token.\n",
    "                # También se considera que el salto de línea se situa después de un signo de puntuación o de un espacio.\n",
    "                if seq[i] == \"\\n\":\n",
    "                    ant = i + 1\n",
    "                \n",
    "        return seqToken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19facda9",
   "metadata": {},
   "source": [
    "### Verificación de la puntuación\n",
    "\n",
    "En la función *verifyPunctuation* se incluyen 3 funciones, que se ejecutan de forma consecutiva:\n",
    "\n",
    "- **Tokenize**: La presentada en la celda anterior, para tokenizar los *strings* de entrada.\n",
    "\n",
    "- **matrizLevenshtein**: Calcula la matriz cuyo último elemento (elemento de coordenadas la última fila y la última columna) es la distancia de Levenshtein. Esta matriz se utiliza para calcular posteriormente la lista de ediciones. \n",
    "\n",
    "- **ediciones**: Calcula la lista de ediciones recorriendo la matriz generada previamente, llamda **M**. **M** se recorre empezando en el último elemento. En cada iteración, mientras *i* y *j* (índices de la matriz de filas y columnas, respectivamente) no sean ambos iguales a 0:\n",
    "    1. Se mira en la vecindad, donde *vecinos* = [**M**(*i -1*, *j*), **M**(*i*, *j - 1*), **M**(*i - 1*, *j - 1*)].\n",
    "    2. Si el valor de algún vecino es igual a **M**(*i*,*j*), se actualizan *j = j - 1* y *i = i - 1*.\n",
    "       Si no, se selecciona el valor mínimo de todos los vecinos, y se añade a la lista de ediciones la operación correspondiente (en función de las coordenadas del vecino de valor mínimo):\n",
    "\n",
    "       - ( *i -1*, *j* ) -> ( 'D',*i* ): Se añade una eliminación a la lista de ediciones. Dicha eliminación se incluye en la lista junto con el índice *i*, que se corresponde con el *string* de *check* en este trabajo. \n",
    "       - ( *i*, *j - 1* ) -> ( 'I',*i* ): Se añade una inserción. \n",
    "       - ( *i -1*, *j - 1* ) -> ( 'S',*i* ): Se añade una sustitución.\n",
    "\n",
    "       \n",
    "       Finalmente, se actualizan *i* y *j* con las coordenadas del vecino seleccionado.\n",
    "\n",
    "Es importante señalar que este algoritmo genera una de las posibles listas de edición que se pueden conseguir, aunque siempre una de distancia mínima. Esto es porque, al recorrer la matriz, si varios vecinos tienen el mismo valor en el paso 2, se ofrece da la posibilidad de escoger varias secuencias de distancia mínima. El algoritmo escogido para este ejercicio da siempre la misma lista de ediciones para dos secuencias iguales.\n",
    "        \n",
    "\n",
    "Inspiración para **matrizLevenshtein**:\n",
    "https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/\n",
    "\n",
    "Inspiración para **ediciones**:\n",
    "https://gist.github.com/curzona/9435822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4810aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def verifyPunctuation(test, check):\n",
    "    ## Primero se tokenizan las dos secuencias mediante la función Tokenize\n",
    "    checkToken = Tokenize(check)\n",
    "    testToken = Tokenize(test)\n",
    "    punt_tuple = (\".\", \",\", \":\", \";\", \"?\", \"!\")\n",
    "            \n",
    "    ## A continuación, se calcula la matriz que computa la distancia de Levenshtein\n",
    "    def matrizLevenshtein(token1, token2):\n",
    "        matrix = np.zeros((len(token1) + 1, len(token2) + 1)) # Se crea la matriz\n",
    "        \n",
    "        for t1 in range(len(token1) + 1): # La primera fila contiene los índices de las posiciones del token 1\n",
    "            matrix[t1][0] = t1\n",
    "        \n",
    "        for t2 in range(len(token2) + 1): # La primera columna contiene los índices de las posiciones del token 2\n",
    "            matrix[0][t2] = t2\n",
    "        \n",
    "        for t1 in range(1, len(token1) + 1):\n",
    "            for t2 in range(1, len(token2) + 1):\n",
    "                # Calcuate \n",
    "                if (token1[t1-1] == token2[t2-1]):\n",
    "                    matrix[t1][t2] = matrix[t1 - 1][t2 - 1]\n",
    "                else:\n",
    "                    a = matrix[t1][t2 - 1]\n",
    "                    b = matrix[t1 - 1][t2]\n",
    "                    c = matrix[t1 - 1][t2 - 1]\n",
    "\n",
    "                    if (a <= b and a <= c):\n",
    "                        matrix[t1][t2] = a + 1\n",
    "                    elif (b <= a and b <= c):\n",
    "                        matrix[t1][t2] = b + 1\n",
    "                    else:\n",
    "                        matrix[t1][t2] = c + 1 \n",
    "\n",
    "        return matrix\n",
    "    matrix_L = matrizLevenshtein(checkToken,testToken)\n",
    "    ## Se calcula la lista de ediciones\n",
    "    def ediciones(token1, token2, mat):\n",
    "        i,j = len(token1), len(token2)\n",
    "        ediciones = []\n",
    "\n",
    "        while(not (i==0 and j==0)):\n",
    "            p = mat[i][j]\n",
    "            vecinos = []\n",
    "\n",
    "            if (i!=0 and j!=0):\n",
    "                vecinos.append(mat[i-1][j-1])\n",
    "\n",
    "            if (i!=0):\n",
    "                vecinos.append(mat[i-1][j])\n",
    "            \n",
    "            if (j!=0):\n",
    "                vecinos.append(mat[i][j-1])\n",
    "            \n",
    "            min_c = min(vecinos)\n",
    "\n",
    "            if(min_c == p): # No se añadie ninguna edición\n",
    "                i, j = i-1, j-1\n",
    "\n",
    "            elif vecinos.count(min_c) > 1: # Si hay más de una posibilidad en los cambios\n",
    "                if ( (token1[i-1] not in punt_tuple and token2[j-1] not in punt_tuple) or (token1[i-1] in punt_tuple and token2[j-1] in punt_tuple)) and i!=0 and j!=0:\n",
    "                    i, j = i-1, j-1\n",
    "                    ediciones.append(('S', i))\n",
    "                elif token1[i-1] in punt_tuple and token2[j-1] not in punt_tuple and i!=0:\n",
    "                    i, j = i-1, j\n",
    "                    ediciones.append(('D', i))\n",
    "                elif token1[i-1] not in punt_tuple and token2[j-1] in punt_tuple and j!= 0:\n",
    "                    i, j = i, j-1\n",
    "                    ediciones.append(('I', i))\n",
    "                else:\n",
    "                    # print(\"Error\")\n",
    "                    # print(vecinos)\n",
    "                    # print(p)\n",
    "                    # print(ediciones)\n",
    "                    # print(check)\n",
    "                    # print(test)\n",
    "                    # print(token1[i])\n",
    "                    # print(token2[j-1])\n",
    "                    return 0\n",
    "\n",
    "            elif(j!=0 and min_c == mat[i][j-1]): # Si solo hay una posibilidad en los cambios, se selecciona\n",
    "                i, j = i, j-1\n",
    "                ediciones.append(('I', i))\n",
    "            elif(i!=0 and j!=0 and min_c == mat[i-1][j-1]):\n",
    "                i, j = i-1, j-1\n",
    "                ediciones.append(('S', i))\n",
    "            elif(i!=0 and min_c == mat[i-1][j]):\n",
    "                i, j = i-1, j\n",
    "                ediciones.append(('D', i))\n",
    "\n",
    "        ediciones.reverse() # Como se ha recorrido la matriz a la inversa, se debe invertir la lista para que esté en el orden adecuado\n",
    "        return ediciones\n",
    "\n",
    " \n",
    "    return ediciones(checkToken, testToken, matrix_L) , testToken, checkToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d57702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('D', 1), ('D', 3), ('I', 7), ('S', 7), ('D', 8), ('I', 15), ('S', 15), ('D', 22), ('S', 24)]\n",
      "Okay well the issue then, Is do we need to be alive. To see this kind of spontaneous order and i've already hinted that the answer is no.\n",
      "Okay, well, the issue then is, do we need to be alive to see this kind of spontaneous order, and I've already hinted that the answer is no. \n"
     ]
    }
   ],
   "source": [
    "check = \"Okay, well, the issue then is, do we need to be alive to see this kind of spontaneous order, and I've already hinted that the answer is no. \"\n",
    "test = \"Okay well the issue then, Is do we need to be alive. To see this kind of spontaneous order and i've already hinted that the answer is no.\"\n",
    "# check = \"Hello. What’s your name?\"\n",
    "# test = \"Hello what’s, Your, name?\"\n",
    "\n",
    "# check  = \"He had the expectations of his senatorial father and Washington, D.C.\"\n",
    "# test = \"He had the expectations of his senatorial father and washington dc.\"\n",
    "ediciones, b, c = verifyPunctuation(test, check)\n",
    "\n",
    "print(ediciones)\n",
    "print(test)\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b700b",
   "metadata": {},
   "source": [
    "A continuación, se comprueba que la función genera una lista de ediciones adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a87feb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 4), ('D', 5), ('I', 7), ('S', 7), ('S', 8)]\n"
     ]
    }
   ],
   "source": [
    "edit,a,b = verifyPunctuation(\"Hello, I am. Francisco and. You.\", \"Hello, I am Francisco, and you?\")\n",
    "print(edit)\n",
    "\n",
    "# edit = verifyPunctuation(\"And? If so how do you explain it.\", \"And, if so, how, do you explain it?\")\n",
    "# print(edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b66a23",
   "metadata": {},
   "source": [
    "## Apartado 3\n",
    " En este apartado se implementa una función (*calculaMetricas*) que calcula el rendimiento de un algoritmo de puntuación, evaluando su resultado en un corpus sobre el que se itera frase a frase. Las métricas que genera son Precisión, *Recall* y F1. También se prueba dicha función para el algoritmo básico de puntuación del apartado 1 sobre el corpus de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ba761",
   "metadata": {},
   "source": [
    "Lo primero que se hace es leer los corpus de *test* y *check* línea a línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9cf5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Guardamos los textos de test y check por líneas\n",
    "f_test = open(\"datasets/PunctuationTask.test.en\",\"r\")\n",
    "f_check = open(\"datasets/PunctuationTask.check.en\",\"r\")\n",
    "linesTest = f_test.readlines()\n",
    "linesCheck = f_check.readlines()\n",
    "f_test.close()\n",
    "f_check.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96008a2c",
   "metadata": {},
   "source": [
    "A continuación, se calculan las métricas correspondientes. Para ello, es necesario definir los conceptos de TP (*True Positives*), FP (*False Positives*) y FN (*False Negatives*) para un modelo de puntuación. Definimos:\n",
    "\n",
    "-- **TP**: Cambios correctos que hace el modelo. Estos pueden ser, en general, para los modelos de puntuación que consideramos: Poner mayúscula, poner punto, poner coma, poner dos puntos, poner punto y coma, poner interrogación, poner exclamación. Es decir, añade el signo de puntuación correcto o la mayúscula donde debía realizarse dicho cambio.\n",
    "\n",
    "-- **FP**: Cambios incorrectos que hace el modelo cuando no se debía hacer un cambio.\n",
    "\n",
    "-- **FN**: Cambios que omite el modelo que deberían haberse realizado.\n",
    "\n",
    "**NOTA**: Es importante tener en cuenta que, con estas definiciones, si el modelo realiza un cambio erróneo donde había que realizar un cambio, este cambio no se categoriza como **FP**, ni como **TP**. Esto parece un buen criterio, ya que no afecta tan negativamente al resultado del modelo como un **FP** (pues no empeora la distancia de Levenshtein). \n",
    "\n",
    "Por ejemplo: Si el *string* check es \"*Qué buen día hace!*\" y el *string* test es \"qué buen día hace\", realizar el cambio \"qué buen día hace.\" no se considera como un **TP**, pues no ha realizado el cambio correcto. Sin embargo, tampoco se considera un **FP**, pues debía realizarse un cambio en esa posición. Contrariamente, si el cambio realizado fuese \"qué buen día. hace\", este supondría un **FP**, ya que se ha insertado un cambio incorrecto en una posición en la que no se debía realizar ningún cambio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e480737",
   "metadata": {},
   "source": [
    "Para calcular los TP, FP, FN, se sigue el razonamiento ilustrado en la figura.\n",
    "\n",
    "Si se observan los bloques verdes, estos se corresponden *strings*: con la frase de test original, la frase de check, y la frase que se obtiene al aplicarle el modelo de puntuación. Los bloques azules se corresponden con la distancia de Levenstein entre pares de *strings*. \n",
    "\n",
    "Los bloques *verifyPunctuation()* reciben como entrada dos *strings* y devuelven la lista de edición entre ambos. Los bloques *len()* devuelven la longitud de la lista que reciben como entrada, y el bloque *model()* genera los signos de puntuación sobre el *string* de entrada.\n",
    "\n",
    "- Si se compara el *string* de test con el *string* de Check, se obtiene el número de cambios totales que debe hacer el modelo, esto es **Db = TP + FN**. \n",
    "\n",
    "- Si se compara el *string* de test con el *string* del modelo, se obtiene el número de cambios totales que hace el modelo, esto es **Dm = TP + FP**. \n",
    "\n",
    "- Si se compara el *string* del modelo con el *string* de Check, ocurre que:\n",
    "  - **Dcheck** será el resultado de sumar: \n",
    "    1. Los **FN**, pues son los cambios que el modelo no ha detectado, cuyas diferencias se siguen manifestado entre ambos *strings*.\n",
    "    2. Los **FP**, pues son los cambios que el modelo ha hecho indebidamente, que empeoran la distancia de Levenshtein.\n",
    "\n",
    "**TP**, **FP** y **FN** son incógnitas y **Dm**, **Db** y **Dcheck** son conocidas, por lo que se tiene un sistema de ecuaciones de tres incógnitas compatible determinado que permite conocer dichas incógnitas.\n",
    "\n",
    "**NOTA**: Es importante destacar que, si se conoce el número de cambios que realiza el modelo, **Dm**, no es necesatio generar dicho número por comparación de strings. Sin embargo, para hacer esta función de evaluación válida para modelos tipo caja negra, se ha decidido computar **Dm** de esta manera.\n",
    "\n",
    "![Diagrama explicativo](Diagrama_PLN.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d32139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Función para calcular las métricas\n",
    "def calculaMetricas(linesTest, linesCheck, model):\n",
    "    ## Iniciaizamos los valores de TP, FP, FN y la matriz de confusión\n",
    "    confusion_m = np.zeros((8,8), dtype=int)\n",
    "    punt_dict = {\n",
    "    \"m\": 0, \"M\": 1, \".\": 2, \",\": 3, \":\": 4, \";\": 5, \"?\": 6, \"!\": 7\n",
    "    }\n",
    "    TP = [0] * 7\n",
    "    TP_FP = [0] * 7\n",
    "    TP_FN = [0] * 7\n",
    "    for (line_test, line_check) in zip(linesTest,linesCheck):\n",
    "        # Aplicamos el modelo de puntuacion\n",
    "        modificado = model(line_test)\n",
    "\n",
    "        cambios_finales, testToken, checkToken = verifyPunctuation(modificado,line_check)\n",
    "        # print(cambios_finales)\n",
    "        pos_finales = [c[1] for c in cambios_finales]\n",
    "        cambios_necesarios  = verifyPunctuation(line_test,line_check)[0]\n",
    "\n",
    "        correctos = [c for c in cambios_necesarios if c[1] not in pos_finales]\n",
    "        # print(cambios_necesarios)\n",
    "        # print(correctos)\n",
    "        # print(cambios_finales)\n",
    "        # print(\"\\n\")\n",
    "        for c in correctos:\n",
    "            if c[0] == 'S' and checkToken[c[1]][0].isupper():\n",
    "                confusion_m[punt_dict[\"M\"] , punt_dict[\"M\"]]  += 1 # Se ha colocado una mayúscula correctamente (se entiende que el test solo contiene minúsculas)\n",
    "            else:\n",
    "                # print(testToken)\n",
    "                # print(checkToken)\n",
    "                # print(cambios_finales)\n",
    "                if checkToken[c[1]] in punt_dict:\n",
    "                    confusion_m[ punt_dict[checkToken[c[1]]] , punt_dict[checkToken[c[1]]] ] += 1 # Se ha insertado el signo correctamente\n",
    "        \n",
    "        offset = 0 # Porque los cambios están referidos al check\n",
    "        for c in cambios_finales:\n",
    "            if c[0] == 'S':\n",
    "\n",
    "                if checkToken[c[1]][0].isupper(): key_c = \"M\"\n",
    "                elif checkToken[c[1]] in punt_dict: key_c = checkToken[c[1]]\n",
    "                else: key_c = \"m\"\n",
    "\n",
    "                if testToken[c[1] + offset][0].isupper(): key_t = \"M\"\n",
    "                elif testToken[c[1] + offset] in punt_dict: key_t = testToken[c[1] + offset]\n",
    "                else: key_t = \"m\"\n",
    "                \n",
    "                if key_t == key_c:\n",
    "                    continue # No se ha cometido un fallo de puntuación\n",
    "\n",
    "                confusion_m[ punt_dict[key_c] , punt_dict[key_t] ] += 1 #[Valor real , Valor predicho]\n",
    "\n",
    "            elif c[0] == 'D':\n",
    "                if checkToken[c[1]] in punt_dict: # Se pone el if ya que en algunos casos la edición real no es la de distancia mínima\n",
    "                    confusion_m[ punt_dict[checkToken[c[1]]] , punt_dict[\"m\"] ] += 1 #[Valor real , Valor predicho]\n",
    "                offset -= 1\n",
    "                \n",
    "            elif c[0] == 'I':\n",
    "                if testToken[c[1] + offset] in punt_dict:\n",
    "                    confusion_m[ punt_dict[\"m\"] , punt_dict[testToken[c[1] + offset]] ] += 1 #[Valor real , Valor predicho]\n",
    "                offset += 1\n",
    "\n",
    "    for i in range(len(TP)):\n",
    "        TP[i] = confusion_m[i+1][i+1]\n",
    "        TP_FP[i] = sum(confusion_m[:,i+1])\n",
    "        TP_FN[i] = sum(confusion_m[i+1,:])\n",
    "        \n",
    "    TP_total = sum(TP)\n",
    "    TP_FP_total = sum(TP_FP)\n",
    "    TP_FN_total = sum(TP_FN)\n",
    "    \n",
    "    return confusion_m, TP, TP_FP, TP_FN, TP_total, TP_FP_total, TP_FN_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0e44e",
   "metadata": {},
   "source": [
    "Finalmente, mediante la función anterior, se calcula la precisión del modelo básico sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aea2034",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linesTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3819/2198398052.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP_FP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP_FN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP_FP_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP_FN_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculaMetricas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinesTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinesCheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddPunctuationBasic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpunt_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"m\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"?\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"!\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunt_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linesTest' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix, TP, TP_FP, TP_FN, TP_total, TP_FP_total, TP_FN_total = calculaMetricas(linesTest, linesCheck, addPunctuationBasic)\n",
    "punt_dict = {\"m\": 0, \"M\": 1, \".\": 2, \",\": 3, \":\": 4, \";\": 5, \"?\": 6, \"!\": 7}\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in punt_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "\n",
    "for i , (tp, tp_fp, tp_fn) in enumerate(zip(TP, TP_FP, TP_FN)):\n",
    "    if tp_fp > 0:\n",
    "        p = tp/tp_fp\n",
    "    else:\n",
    "        p = 0\n",
    "    if tp_fn > 0:\n",
    "        r = tp/tp_fn\n",
    "    else:\n",
    "        r = 0\n",
    "    if not (p == 0 and r == 0):\n",
    "        f1 = 2 * (p*r) / (p+r)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    if tp_fp > 0:\n",
    "        print(\"Precision para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(p))\n",
    "    else:\n",
    "        print(\"Este modelo no ha añadido este símbolo\")\n",
    "\n",
    "    if tp_fn > 0:\n",
    "        print(\"Recall para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(r))\n",
    "    else:\n",
    "        print(\"En este corpus no aparece este símbolo\")\n",
    "\n",
    "    print(\"F1 para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(f1))\n",
    "    print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "if TP_FP_total > 0:\n",
    "    P = TP_total/TP_FP_total\n",
    "    print(\"Precision total -> \" + str(P))\n",
    "\n",
    "if tp_fn > 0:\n",
    "    R = TP_total/TP_FN_total\n",
    "    print(\"Recall total -> \" + str(R))\n",
    "\n",
    "print(\"F1 total -> \" + str(2 * (P*R)/(P+R)))\n",
    "print(\"--------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba7e95",
   "metadata": {},
   "source": [
    "## Apartado 4\n",
    "\n",
    "En este apartado se realizan dos tareas: \n",
    "\n",
    "- Se crea un modelo de puntuación basado en pseudo-cuatrogramas: Este modelo decidirá, en función de las tres palabras anteriores, si realizar una de las siguientes operaciones: Poner la palabra en mayúscula, poner la palabra en minúscula o añadir un signo de puntuación.\n",
    "\n",
    "- Una función que, utilizando dicho modelo, realice la operación adecuada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30f360",
   "metadata": {},
   "source": [
    "El modelo se genera a partir de un texto tokenizado, realizando los siguientes pasos:\n",
    "\n",
    "1. Se crean cuatrogramas de la siguiente forma: (**token1**, **token2**, **token3**, **operación**). Esto se hace recorriendo el texto tokenizado de cuatro en cuatro y, para el último token, sustituir el contenido por la "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b66c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudoCuatroGramas(Token):\n",
    "    # Creamos una lista con los pseudo 4-gramas\n",
    "    cuatroGramas = []\n",
    "    for i in range(4, len(Token)):\n",
    "        c_g = Token[(i - 4):i]   \n",
    "        if c_g[-1][0].islower():\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 0])\n",
    "        elif c_g[-1][0].isupper():\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 1])\n",
    "        elif c_g[-1][0] == '.':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 2])\n",
    "        elif c_g[-1][0] == ',':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 3])\n",
    "        elif c_g[-1][0] == ':':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 4])\n",
    "        elif c_g[-1][0] == ';':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 5])\n",
    "        elif c_g[-1][0] == '?':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 6])\n",
    "        elif c_g[-1][0] == '!':\n",
    "            cuatroGramas.append([tuple(Token[i - 4:i-1]), 7])\n",
    "    # print(cuatroGramas[:200])\n",
    "    return cuatroGramas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open(\"datasets/PunctuationTask.train.en\",\"r\")\n",
    "linesTrain = f_train.read() # Guardamos el conjunto de entrenamiento\n",
    "f_train.close()\n",
    "trainToken = Tokenize(linesTrain)\n",
    "cuatroGramas = pseudoCuatroGramas(trainToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "314efb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un diccionario para almacenar los tokens únicos y su valor\n",
    "fDist = dict()\n",
    "for cg in cuatroGramas:\n",
    "    if tuple(cg) in fDist:\n",
    "        fDist[tuple(cg)] += 1\n",
    "    else:\n",
    "        fDist[tuple(cg)] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a21f96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se crea la función que añade los signos de puntuación\n",
    "def addPunctuation4gram(sent, addMayusc = True, addDot = True):\n",
    "    sentToken = Tokenize(sent)\n",
    "\n",
    "    # Para añadir la primera palabra en mayúscula\n",
    "    if addMayusc:\n",
    "        sentToken[0] = sentToken[0].capitalize()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentToken) - 2:\n",
    "        valor = tuple(sentToken[i:i + 3])\n",
    "        max = 0\n",
    "        max_index = 0\n",
    "        flag = False\n",
    "        for j in range(8):\n",
    "            if tuple([valor, j]) in fDist:\n",
    "                flag = True\n",
    "                act = fDist[tuple([valor, j])]\n",
    "            else:\n",
    "                continue\n",
    "            if act > max:\n",
    "                max = act\n",
    "                max_index = j\n",
    "        \n",
    "        if flag == True:\n",
    "            # Dependiendo del valor máximo, se eliige una acción u otra\n",
    "            if max_index == 0 and (i + 3) < len(sentToken) and not sentToken[i+3].islower():\n",
    "                sentToken[i+3] = sentToken[i+3][0].lower() + sentToken[i+3][1:]\n",
    "            if max_index == 1 and (i + 3) < len(sentToken):\n",
    "                sentToken[i+3] = sentToken[i+3].capitalize() \n",
    "            elif max_index == 2:\n",
    "                sentToken.insert(i+3, '.') \n",
    "            elif max_index == 3:\n",
    "                sentToken.insert(i+3, ',') \n",
    "            elif max_index == 4:\n",
    "                sentToken.insert(i+3, ':')    \n",
    "            elif max_index == 5:\n",
    "                sentToken.insert(i+3, ';') \n",
    "            elif max_index == 6:\n",
    "                sentToken.insert(i+3, '?') \n",
    "            elif max_index == 7:\n",
    "                sentToken.insert(i+3, '!')\n",
    "            elif addDot and (i + 3) == len(sentToken) and sentToken[i+2] not in ['.', '?', '!']:\n",
    "                sentToken.insert(i+3, '.') \n",
    "        elif addDot and (i + 3) == len(sentToken) and sentToken[i+2] not in ['.', '?', '!']:\n",
    "            sentToken.insert(i+3, '.')\n",
    "\n",
    "        i += 1\n",
    "    sentReturn = ''\n",
    "    for k, i in enumerate(sentToken):\n",
    "        if i == ',' or i == '.' or i == ':' or i == ';' or i == '?' or i == '!' or k == 0:\n",
    "            sentReturn = sentReturn + i\n",
    "        else:\n",
    "            sentReturn = sentReturn + ' ' + i\n",
    "    return sentReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa6874",
   "metadata": {},
   "source": [
    "## Apartado 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "549b0a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision para la operación añadir (M) -> 0.8422603865555744\n",
      "Recall para la operación añadir (M) -> 0.5466853064527997\n",
      "F1 para la operación añadir (M) -> 0.6630228667038484\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (.) -> 0.7986943694683322\n",
      "Recall para la operación añadir (.) -> 0.8731814438649464\n",
      "F1 para la operación añadir (.) -> 0.8342785955479789\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (,) -> 0.3995123650296064\n",
      "Recall para la operación añadir (,) -> 0.07046753087178227\n",
      "F1 para la operación añadir (,) -> 0.11980363484437018\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (:) -> 0.14285714285714285\n",
      "Recall para la operación añadir (:) -> 0.01020408163265306\n",
      "F1 para la operación añadir (:) -> 0.019047619047619042\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (;) -> 0.03125\n",
      "Recall para la operación añadir (;) -> 0.0036900369003690036\n",
      "F1 para la operación añadir (;) -> 0.006600660066006601\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (?) -> 0.3333333333333333\n",
      "Recall para la operación añadir (?) -> 0.05358550039401103\n",
      "F1 para la operación añadir (?) -> 0.0923285811269518\n",
      "--------------------------------------------------------------------------\n",
      "Precision para la operación añadir (!) -> 0.3333333333333333\n",
      "Recall para la operación añadir (!) -> 0.030303030303030304\n",
      "F1 para la operación añadir (!) -> 0.05555555555555555\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, TP, TP_FP, TP_FN = calculaMetricas(linesTest, linesCheck, addPunctuation4gram)\n",
    "punt_dict = {\"m\": 0, \"M\": 1, \".\": 2, \",\": 3, \":\": 4, \";\": 5, \"?\": 6, \"!\": 7}\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in punt_dict.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "\n",
    "for i , (tp, tp_fp, tp_fn) in enumerate(zip(TP, TP_FP, TP_FN)):\n",
    "    if tp_fp > 0:\n",
    "        p = tp/tp_fp\n",
    "    else:\n",
    "        p = 0\n",
    "    if tp_fn > 0:\n",
    "        r = tp/tp_fn\n",
    "    else:\n",
    "        r = 0\n",
    "    if not (p == 0 and r == 0):\n",
    "        f1 = 2 * (p*r) / (p+r)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    if tp_fp > 0:\n",
    "        print(\"Precision para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(p))\n",
    "    else:\n",
    "        print(\"Este modelo no ha añadido este símbolo\")\n",
    "\n",
    "    if tp_fn > 0:\n",
    "        print(\"Recall para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(r))\n",
    "    else:\n",
    "        print(\"En este corpus no aparece este símbolo\")\n",
    "\n",
    "    print(\"F1 para la operación añadir (\" + str(get_key(i+1)) + \") -> \" + str(f1))\n",
    "    print(\"--------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de602589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7728151170281297\n",
      "Recall: 0.46585981489871203\n",
      "F1: 0.5813042600444175\n"
     ]
    }
   ],
   "source": [
    "TP = 0 \n",
    "FN = 0\n",
    "FP = 0\n",
    "\n",
    "for (line1, line2) in zip(linesTest,linesCheck):\n",
    "    evaluation = calculaMetricas(line1, line2, addPunctuation4gram)\n",
    "    TP += evaluation[0]\n",
    "    FN += evaluation[1]\n",
    "    FP += evaluation[2]\n",
    "\n",
    "P = TP / (TP + FP)\n",
    "R = TP / (TP + FN)\n",
    "F1 = 2 * (P * R) / (P + R)\n",
    "\n",
    "print(\"Precision: \" + str(P))\n",
    "print(\"Recall: \" + str(R))\n",
    "print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea369256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mix_model(sent):\n",
    "#     sent2 = addPunctuation4gram(sent)\n",
    "#     return addPunctuationBasic(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = 0 \n",
    "# R = 0\n",
    "# F1 = 0\n",
    "\n",
    "# for (line1, line2) in zip(linesTest,linesCheck):\n",
    "#     evaluation = calculaMetricas(line1, line2, mix_model)\n",
    "#     P += evaluation[0]\n",
    "#     R += evaluation[1]\n",
    "#     F1 += evaluation[2]\n",
    "\n",
    "# P /= len(linesTest)\n",
    "# R /= len(linesTest)\n",
    "# F1 /= len(linesTest)\n",
    "\n",
    "# print(\"Precision: \" + str(P))\n",
    "# print(\"Recall: \" + str(R))\n",
    "# print(\"F1: \" + str(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a266a",
   "metadata": {},
   "source": [
    "## Apartado 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9091dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos los textos para que puedan ser leídos por el modelo\n",
    "# f_test = open(\"datasets/PunctuationTask.test.en\",\"r\")\n",
    "# f_check = open(\"datasets/PunctuationTask.check.en\",\"r\")\n",
    "f_train = open(\"datasets/PunctuationTask.train.en\",\"r\")\n",
    "Test = f_test.read()\n",
    "Check = f_check.read()\n",
    "Train = f_train.read()\n",
    "f_test.close()\n",
    "f_check.close()\n",
    "f_train.close()\n",
    "\n",
    "Check_new = \"\"\n",
    "for c in Check:\n",
    "    if c == ',' or c == '.' or c == ':' or c == ';' or c == '?' or c == '!':\n",
    "        Check_new += \" \"\n",
    "    Check_new += c\n",
    "\n",
    "Train_new = \"\"\n",
    "for c in Train:\n",
    "    if c == ',' or c == '.' or c == ':' or c == ';' or c == '?' or c == '!':\n",
    "        Train_new += \" \"\n",
    "    Train_new += c\n",
    "\n",
    "\n",
    "with open('datasets/test.txt', 'w') as f:\n",
    "    f.write(Test.lower())\n",
    "f.close()\n",
    "with open('datasets/dev.txt', 'w') as f:\n",
    "    f.write(Check_new.lower())\n",
    "f.close()\n",
    "with open('datasets/train.txt', 'w') as f:\n",
    "    f.write(Train_new.lower())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2406e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grvc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Skipped 594 lines\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!mkdir paraModelo\n",
    "!python3 Tokenization_Model.py datasets/PunctuationTask.train.en paraModelo/Total.txt\n",
    "\n",
    "import math\n",
    "\n",
    "perc_train = 0.8\n",
    "perc_val = 0.2\n",
    "f_total = open(\"paraModelo/Total.txt\",\"r\")\n",
    "total_lines = f_total.readlines()\n",
    "f_total.close()\n",
    "\n",
    "trainSize = math.floor(0.8 * len(total_lines))\n",
    "f = open(\"paraModelo/train.txt\",\"w\")\n",
    "f.writelines(total_lines[:trainSize])\n",
    "f.close()\n",
    "\n",
    "f = open(\"paraModelo/val.txt\", \"w\")\n",
    "f.writelines(total_lines[trainSize:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c106abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grvc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Skipped 62 lines\n"
     ]
    }
   ],
   "source": [
    "# procesamos el CHECK par pasarselo al modelo y que compruebe\n",
    "!python3 Tokenization_Check.py datasets/PunctuationTask.check.en paraModelo/check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68a6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/grvc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Skipped 62 lines\n"
     ]
    }
   ],
   "source": [
    "!python3 Tokenization_Test.py datasets/PunctuationTask.check.en paraModelo/check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1494bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35340/3600999069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpunctuator2tf2FJGO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# vocab_len = len(data.read_vocabulary(data.WORD_VOCAB_FILE))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# x_len = vocab_len if vocab_len < data.MAX_WORD_VOCABULARY_SIZE else data.MAX_WORD_VOCABULARY_SIZE + data.MIN_WORD_COUNT_IN_VOCAB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# x = np.ones((x_len, main.MINIBATCH_SIZE)).astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/EJERCICIO_PLN/EJERCICIO_PLN/punctuator2tf2FJGO/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from punctuator2tf2FJGO import models, data, main\n",
    "vocab_len = len(data.read_vocabulary(data.WORD_VOCAB_FILE))\n",
    "x_len = vocab_len if vocab_len < data.MAX_WORD_VOCABULARY_SIZE else data.MAX_WORD_VOCABULARY_SIZE + data.MIN_WORD_COUNT_IN_VOCAB\n",
    "x = np.ones((x_len, main.MINIBATCH_SIZE)).astype(int)\n",
    "\n",
    "print(\"Loading model parameters...\")\n",
    "net, _ = models.load(model_file, x)\n",
    "\n",
    "print(\"Building model...\")\n",
    "\n",
    "word_vocabulary = net.x_vocabulary\n",
    "punctuation_vocabulary = net.y_vocabulary\n",
    "\n",
    "reverse_word_vocabulary = {v:k for k,v in word_vocabulary.items()} # Dado un valor, me devuelve su clave (palabra)\n",
    "reverse_punctuation_vocabulary = {v:k for k,v in punctuation_vocabulary.items()} # Dado un valor, me devuelve su clave (signo de puntuación)\n",
    "\n",
    "with codecs.open(input_file, 'r', 'utf-8') as f:\n",
    "    input_text = f.readlines() # read()\n",
    "\n",
    "if len(input_text) == 0:\n",
    "    sys.exit(\"Input file empty.\")\n",
    "\n",
    "text = [line.split() for line in input_text]\n",
    "# for i,t in enumerate(text):\n",
    "#     if t not in punctuation_vocabulary and t not in data.PUNCTUATION_MAPPING:\n",
    "#         pass\n",
    "#     else:\n",
    "#         text.pop(i)\n",
    "\n",
    "#print(text)\n",
    "#text = [w for w in input_text.split() if w not in punctuation_vocabulary and w not in data.PUNCTUATION_MAPPING] + [data.END]\n",
    "#text = [w for w in input_text.split() if w not in punctuation_vocabulary and w not in data.PUNCTUATION_MAPPING] + [data.END]\n",
    "print(restore(text, word_vocabulary, reverse_punctuation_vocabulary, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45aa92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo de puntuación de los autores\n",
    "def restore(text_lines, word_vocabulary, reverse_punctuation_vocabulary, model):\n",
    "    i = 0\n",
    "    puntuated = ''\n",
    "    text_line = []\n",
    "    [text_line.append(w) for frase in text_lines for w in frase]\n",
    "    print(text_line)\n",
    "    print(\"Processing line\")\n",
    "    print(text_line)\n",
    "    if len(text_line) == 0:\n",
    "        return\n",
    "\n",
    "    # Si la palabra aparece en el vovabulario, se le pasa a la red, si no, se le pasa el token para caracter desconocido\n",
    "    converted_subsequence = [word_vocabulary.get(w, word_vocabulary[data.UNK]) for w in text_line]\n",
    "\n",
    "    # Predicción del modelo\n",
    "    y = predict(to_array(converted_subsequence), model)\n",
    "    puntuated = puntuated + (text_line[0])\n",
    "\n",
    "    last_eos_idx = 0\n",
    "    punctuations = []\n",
    "    for y_t in y:\n",
    "\n",
    "        p_i = np.argmax(tf.reshape(y_t, [-1]))\n",
    "        punctuation = reverse_punctuation_vocabulary[p_i]\n",
    "\n",
    "        punctuations.append(punctuation)\n",
    "\n",
    "        if punctuation in data.EOS_TOKENS:\n",
    "            last_eos_idx = len(punctuations) # we intentionally want the index of next element\n",
    "    print(punctuations)\n",
    "    # if text_line[-1] == data.END:\n",
    "    #     step = len(text_line) - 1\n",
    "    # elif last_eos_idx != 0:\n",
    "    #     step = last_eos_idx\n",
    "    # else:\n",
    "    step = len(text_line) - 1\n",
    "\n",
    "    for j in range(step):\n",
    "        puntuated = puntuated + (\" \" + punctuations[j] + \" \" if punctuations[j] != data.SPACE else \" \")\n",
    "        if j < step - 1:\n",
    "            puntuated = puntuated + (text_line[1+j])\n",
    "\n",
    "    puntuated = puntuated + (text_line[-1])\n",
    "\n",
    "    print(puntuated)\n",
    "    print(\"All lines processed\")\n",
    "    return puntuated\n",
    "\n",
    "def predict(x, model):\n",
    "    return tf.nn.softmax(net(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "55c4f13588671055eddbcd88b1478edc188b631075a8eed8f2c844334309e5e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
